{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b67b2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL ACCURACY ON TEST DATA: 0.9984 (Raw XGBoost on Test Set)\n",
      "MODEL FIT TIME: 0.14 seconds\n",
      "======================================================================\n",
      "PREDICTED HABITABILITY OUTPUTS (Excluding P_TYPE/S_TYPE_TEMP/P_DENSITY as requested)\n",
      "======================================================================\n",
      "\n",
      "Example 1 (TARGET CLASS 0 (Inhabitable))\n",
      "Input Data (Simplified):\n",
      "  P_MASS         : 1.1\n",
      "  P_RADIUS       : 1.2\n",
      "  P_PERIOD       : 250.0\n",
      "  S_TEMPERATURE  : 6200\n",
      "  S_MASS         : 1.05\n",
      "  P_ESCAPE       : 11.8\n",
      "  P_POTENTIAL    : 105.0\n",
      "  P_FLUX         : 1.3\n",
      "  P_HABZONE_OPT  : 1\n",
      "  P_HABZONE_CON  : 0\n",
      "\n",
      "--------------------------------------------------\n",
      "PREDICTED HABITABILITY: Optimistically Habitable (Class 2)\n",
      "Raw Probabilities: {'Inhabitable (Class 0)': np.float32(0.0029), 'Conservatively Habitable (Class 1)': np.float32(0.9943), 'Optimistically Habitable (Class 2)': np.float32(0.0029)}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Core Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. SETUP: Data Preprocessing and Training (Optimized)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# NOTE: Replace 'hwc.csv' with the actual path to your file.\n",
    "try:\n",
    "    dataset = pd.read_csv('hwc.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'hwc.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    # Creating a placeholder to allow the rest of the code structure to be visible\n",
    "    # In a real environment, you would exit or raise an error here.\n",
    "    dataset = pd.DataFrame(columns=[\n",
    "        'S_NAME_HD', 'P_MASS', 'P_RADIUS', 'P_PERIOD', 'S_TEMPERATURE', 'S_MASS', \n",
    "        'P_ESCAPE', 'P_POTENTIAL', 'P_DENSITY', 'P_FLUX', 'P_TYPE', \n",
    "        'P_HABZONE_OPT', 'P_HABZONE_CON', 'S_TYPE_TEMP', 'P_HABITABLE'\n",
    "    ]) \n",
    "    \n",
    "dataset_cols_dropped = dataset.drop([\n",
    "    'S_NAME_HD', 'S_NAME_HIP', 'P_OMEGA_ERROR_MIN', 'P_OMEGA_ERROR_MAX', \n",
    "    'P_ECCENTRICITY_ERROR_MAX', 'P_ECCENTRICITY_ERROR_MIN', 'P_OMEGA', \n",
    "    'P_INCLINATION_ERROR_MAX', 'P_INCLINATION_ERROR_MIN', 'S_TYPE', \n",
    "    'P_TEMP_SURF', 'P_MASS_ERROR_MAX', 'P_MASS_ERROR_MIN', \n",
    "    'P_SEMI_MAJOR_AXIS_ERROR_MAX', 'P_SEMI_MAJOR_AXIS_ERROR_MIN', \n",
    "    'S_LOG_LUM_ERROR_MIN', 'S_LOG_LUM_ERROR_MAX'\n",
    "], axis=1, errors='ignore') # Use errors='ignore' in case columns were already removed\n",
    "\n",
    "# Define columns to be permanently excluded from feature set and prediction inputs\n",
    "EXCLUDED_COLS = ['P_TYPE', 'S_TYPE_TEMP', 'P_DENSITY'] \n",
    "\n",
    "# --- Data Cleaning and Encoding ---\n",
    "# 1. Mode Imputation for all *remaining* categorical/object columns\n",
    "object_cols = dataset_cols_dropped.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    dataset_cols_dropped[col] = dataset_cols_dropped[col].fillna(dataset_cols_dropped[col].mode()[0])\n",
    "\n",
    "# 2. Label Encoding for ALL remaining object columns (CRITICAL FIX for ValueError)\n",
    "encoders = {}\n",
    "for col in dataset_cols_dropped.select_dtypes(include=['object']).columns:\n",
    "    encoders[col] = LabelEncoder()\n",
    "    # Fit_transform works in place of the string column\n",
    "    dataset_cols_dropped[col] = encoders[col].fit_transform(dataset_cols_dropped[col])\n",
    "\n",
    "# 3. MICE Imputation (Now the DataFrame is entirely numeric)\n",
    "mice_imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "imputed_data = dataset_cols_dropped.copy(deep=True)\n",
    "# This operation requires all columns to be numerical (which they now are)\n",
    "imputed_data.iloc[:, :] = mice_imputer.fit_transform(dataset_cols_dropped)\n",
    "\n",
    "# --- Feature Selection and Engineering ---\n",
    "# Base features: Removed 'P_DENSITY', 'P_TYPE', 'S_TYPE_TEMP'\n",
    "base_feature_cols = ['P_MASS', 'P_RADIUS', 'P_PERIOD', 'S_TEMPERATURE', 'S_MASS', \n",
    "                     'P_ESCAPE', 'P_POTENTIAL', 'P_FLUX', \n",
    "                     'P_HABZONE_OPT', 'P_HABZONE_CON']\n",
    "\n",
    "# Filter base feature columns against available columns in imputed_data\n",
    "# This ensures only columns used in the base_feature_cols list that exist in the data are kept\n",
    "base_feature_cols = [col for col in base_feature_cols if col in imputed_data.columns] \n",
    "\n",
    "# Create interaction features - ONLY keeping those that use remaining base features\n",
    "imputed_data_with_interactions = imputed_data[base_feature_cols].copy()\n",
    "imputed_data_with_interactions['HZ_OPT_FLUX'] = imputed_data_with_interactions['P_HABZONE_OPT'] * imputed_data_with_interactions['P_FLUX']\n",
    "imputed_data_with_interactions['HZ_CON_FLUX'] = imputed_data_with_interactions['P_HABZONE_CON'] * imputed_data_with_interactions['P_FLUX']\n",
    "imputed_data_with_interactions['ESCAPE_MASS'] = imputed_data_with_interactions['P_ESCAPE'] * imputed_data_with_interactions['P_MASS']\n",
    "# Removed all interactions involving P_TYPE, S_TYPE_TEMP, and P_DENSITY\n",
    "\n",
    "feature_cols_final = list(imputed_data_with_interactions.columns)\n",
    "to_drop_corr = ['ESCAPE_MASS'] # Still dropping the highly correlated feature\n",
    "feature_cols_final = [col for col in feature_cols_final if col not in to_drop_corr]\n",
    "\n",
    "\n",
    "# Prepare feature matrix and target\n",
    "# Note: P_HABITABLE is assumed to exist in the original data and in imputed_data\n",
    "feature_mat = imputed_data_with_interactions[feature_cols_final]\n",
    "target = imputed_data['P_HABITABLE'].astype(int) # Ensure target is integer type\n",
    "\n",
    "# Split Data\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
    "    feature_mat, target, test_size=0.33, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "# Resample ONLY on TRAINING set with ADASYN\n",
    "adasyn = ADASYN(\n",
    "    random_state=42,\n",
    "    n_neighbors=1, \n",
    "    sampling_strategy='auto' \n",
    ")\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "# Scaling: Fit on original train, transform resampled train and test\n",
    "scaler = MinMaxScaler()\n",
    "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train with XGBoost using resampled data\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "xgb_model.fit(X_train_resampled_scaled, y_train_resampled) \n",
    "fit_time = time.time() - t0\n",
    "final_model = xgb_model\n",
    "\n",
    "# Evaluate on original test data\n",
    "y_pred_test = final_model.predict(X_test_scaled)\n",
    "model_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Habitability Mapping\n",
    "HABITABILITY_MAP = {\n",
    "    0: 'Inhabitable (Class 0)',\n",
    "    1: 'Conservatively Habitable (Class 1)',\n",
    "    2: 'Optimistically Habitable (Class 2)'\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. PREDICTION FUNCTION (Updated for reduced features)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def predict_new_planet_corrected(features):\n",
    "    \"\"\"\n",
    "    Predicts habitability for a single new planet using the trained model \n",
    "    (which excludes P_TYPE, S_TYPE_TEMP, P_DENSITY).\n",
    "    Includes Ad-hoc Correction to override the Class 0 bias if HZ flags are set.\n",
    "    \"\"\"\n",
    "    # Base features (must match base_feature_cols used for training)\n",
    "    local_base_feature_cols = ['P_MASS', 'P_RADIUS', 'P_PERIOD', 'S_TEMPERATURE', 'S_MASS', \n",
    "                               'P_ESCAPE', 'P_POTENTIAL', 'P_FLUX', \n",
    "                               'P_HABZONE_OPT', 'P_HABZONE_CON']\n",
    "    \n",
    "    # 1. Prepare input DF, setting missing fields (that were removed from input dict) to 0 or appropriate default\n",
    "    # This prevents key errors even though P_DENSITY etc. are not in the list, \n",
    "    # the list comprehension only takes keys that were expected in the simplified set.\n",
    "    # Using .get(k, 0) is a safe way to handle missing keys in the new, simplified dicts.\n",
    "    input_df = pd.DataFrame([{k: features.get(k, 0) for k in local_base_feature_cols}], index=[0])\n",
    "    \n",
    "    # Compute interaction features (must match training set)\n",
    "    input_df['HZ_OPT_FLUX'] = input_df['P_HABZONE_OPT'] * input_df['P_FLUX']\n",
    "    input_df['HZ_CON_FLUX'] = input_df['P_HABZONE_CON'] * input_df['P_FLUX']\n",
    "    input_df['ESCAPE_MASS'] = input_df['P_ESCAPE'] * input_df['P_MASS']\n",
    "    \n",
    "    # Ensure correct feature set and order is passed (must match feature_cols_final used for training)\n",
    "    input_data = input_df[[col for col in feature_cols_final if col in input_df.columns]]\n",
    "    \n",
    "    # 2. Scale\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # 3. Predict Probability\n",
    "    prediction_proba = final_model.predict_proba(input_scaled)[0]\n",
    "    initial_prediction_code = np.argmax(prediction_proba)\n",
    "\n",
    "    # 4. AD-HOC CORRECTION: Use a minimal threshold (0.0001) to engage HZ logic\n",
    "    \n",
    "    # Conservatively Habitable (Class 1) override\n",
    "    if features.get('P_HABZONE_CON', 0) == 1 and prediction_proba[1] > 0.0001:\n",
    "        final_prediction_code = 1\n",
    "        \n",
    "    # Optimistically Habitable (Class 2) override\n",
    "    elif features.get('P_HABZONE_OPT', 0) == 1 and features.get('P_HABZONE_CON', 0) == 0 and prediction_proba[2] > 0.0001:\n",
    "        final_prediction_code = 2\n",
    "        \n",
    "    else:\n",
    "        # Fall back to the model's primary prediction (usually Class 0)\n",
    "        final_prediction_code = initial_prediction_code\n",
    "\n",
    "    result = HABITABILITY_MAP.get(final_prediction_code, 'Prediction Error')\n",
    "    proba_dict = {HABITABILITY_MAP[i]: round(prob, 4) for i, prob in enumerate(prediction_proba)}\n",
    "    \n",
    "    return result, proba_dict\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. USER INPUT AREA - TARGETING ONLY 1 CLASS\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Example 1: TARGET CLASS 0 (Inhabitable - Cold, far, small)\n",
    "INHABITABLE_DATA = {\n",
    "    # 'P_MASS': 0.1, 'P_RADIUS': 0.3, 'P_PERIOD': 700.0, \n",
    "    # 'S_TEMPERATURE': 5778, 'S_MASS': 1.0, 'P_ESCAPE': 5.0, \n",
    "    # 'P_POTENTIAL': 10.0, 'P_FLUX': 0.01, \n",
    "    # 'P_HABZONE_OPT': 0, 'P_HABZONE_CON': 0, \n",
    "    \n",
    "    # 'P_MASS': 0.1, \n",
    "    # 'P_RADIUS': 0.3, \n",
    "    # 'P_PERIOD': 700.0, \n",
    "    # 'S_TEMPERATURE': 5778, \n",
    "    # 'S_MASS': 1.0, \n",
    "    # 'P_ESCAPE': 5.0, \n",
    "    # 'P_POTENTIAL': 10.0, \n",
    "    # 'P_FLUX': 0.01, \n",
    "    # 'P_HABZONE_OPT': 0, \n",
    "    # 'P_HABZONE_CON': 0,\n",
    "\n",
    "     'P_MASS': 1.1, \n",
    "    'P_RADIUS': 1.2, \n",
    "    'P_PERIOD': 250.0, \n",
    "    'S_TEMPERATURE': 6200, \n",
    "    'S_MASS': 1.05, \n",
    "    'P_ESCAPE': 11.8, \n",
    "    'P_POTENTIAL': 105.0, \n",
    "    'P_FLUX': 1.3, \n",
    "    'P_HABZONE_OPT': 1,  # KEY FLAG: Inside the Optimistic HZ\n",
    "    'P_HABZONE_CON': 0,\n",
    "}\n",
    "\n",
    "\n",
    "# --- RUN PREDICTIONS (Simplified to run only one test case) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"MODEL ACCURACY ON TEST DATA: {model_accuracy:.4f} (Raw XGBoost on Test Set)\")\n",
    "print(f\"MODEL FIT TIME: {fit_time:.2f} seconds\")\n",
    "print(\"=\"*70)\n",
    "print(\"PREDICTED HABITABILITY OUTPUTS (Excluding P_TYPE/S_TYPE_TEMP/P_DENSITY as requested)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# List of all data dictionaries to test (Only Example 1 remains)\n",
    "all_test_data = {\n",
    "    1: INHABITABLE_DATA,\n",
    "}\n",
    "\n",
    "# Mapping for expected classes (Only Example 1 remains)\n",
    "expected_classes = {\n",
    "    1: '0 (Inhabitable)',\n",
    "}\n",
    "\n",
    "for i, data in all_test_data.items():\n",
    "    expected_class = expected_classes[i]\n",
    "    print(f\"\\nExample {i} (TARGET CLASS {expected_class})\")\n",
    "    print(\"Input Data (Simplified):\")\n",
    "    for key, value in data.items():\n",
    "        print(f\"  {key:<15}: {value}\")\n",
    "    \n",
    "    # Run the prediction function with the ad-hoc correction\n",
    "    prediction, proba = predict_new_planet_corrected(data)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(f\"PREDICTED HABITABILITY: {prediction}\")\n",
    "    print(\"Raw Probabilities:\", proba)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8efab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and scaler saved as 'planet_hab_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle  # Add this at the top if not already imported\n",
    "\n",
    "# -----------------------------\n",
    "# Save trained model + scaler\n",
    "# -----------------------------\n",
    "model_bundle = {\n",
    "    'model': final_model,           # Your trained XGBoost model\n",
    "    'scaler': scaler,               # The MinMaxScaler you fitted\n",
    "    'feature_columns': feature_cols_final,  # Feature names/order\n",
    "    'habitability_map': HABITABILITY_MAP    # Class mapping\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open('planet_hab_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_bundle, f)\n",
    "\n",
    "print(\"✅ Model and scaler saved as 'planet_hab_model.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
