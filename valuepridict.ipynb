{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0e2c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded: Shape (5599, 118)\n",
      "\n",
      "--- Model Configuration ---\n",
      "Inputs (Minimal Initial + Composition Hint, 6): ['P_PERIOD', 'P_RADIUS', 'P_TEMP_EQUIL', 'S_TEMPERATURE', 'S_MASS'] + P_COMPOSITION_ROCKY\n",
      "Targets (Derived/Hard, 39): P_ECCENTRICITY is included. Other examples: ['P_MASS', 'P_SEMI_MAJOR_AXIS', 'P_ECCENTRICITY', 'P_INCLINATION', 'S_DISTANCE']...\n",
      "Added P_COMPOSITION_ROCKY to training data based on P_DENSITY.\n",
      "\n",
      "Training LightGBM MultiOutputRegressor (Fast Mode)...\n",
      "✅ System components saved to ONE file: 'exoplanet_predictor_system.pkl'\n",
      "\n",
      "=== Model Accuracy Evaluation ===\n",
      "Overall Avg R²: 0.8897\n",
      "\n",
      "==================================================\n",
      "USER-EDITABLE PREDICTION INPUTS\n",
      "==================================================\n",
      "\n",
      "✅ Loaded model system from 'exoplanet_predictor_system.pkl'\n",
      "Expected 7 input features:\n",
      "['P_PERIOD', 'P_RADIUS', 'P_TEMP_EQUIL', 'S_TEMPERATURE', 'S_MASS', 'P_COMPOSITION_ROCKY', 'water_potential_proxy']\n",
      "\n",
      "==================================================\n",
      "PREDICTION REPORT | Overall Model R²: 0.8897\n",
      "==================================================\n",
      "--- INPUTS USED ---\n",
      "[Input]     P_PERIOD             : 300.0000\n",
      "[Input]     P_RADIUS             : 1.4000\n",
      "[Input]     P_TEMP_EQUIL         : 270.0000\n",
      "[Input]     S_TEMPERATURE        : 5778.0000\n",
      "[Input]     S_MASS               : 1.0000\n",
      "[Input]     P_COMPOSITION_ROCKY  : 1.0000\n",
      "[Input]     water_potential_proxy: 1.0000\n",
      "\n",
      "--- PREDICTED OUTPUTS ---\n",
      "[Predicted] P_MASS               : 21.6249\n",
      "[Predicted] P_SEMI_MAJOR_AXIS    : 0.9092\n",
      "[Predicted] P_ECCENTRICITY       : 0.0979\n",
      "[Predicted] P_INCLINATION        : 83.2570\n",
      "[Predicted] S_DISTANCE           : 476.4653\n",
      "[Predicted] S_RADIUS             : 0.9923\n",
      "[Predicted] S_LOG_LUM            : 0.0832\n",
      "[Predicted] S_LOG_G              : 4.4223\n",
      "[Predicted] P_ESCAPE             : 3.7419\n",
      "[Predicted] P_POTENTIAL          : 14.8108\n",
      "[Predicted] P_GRAVITY            : 9.2079\n",
      "[Predicted] P_DENSITY            : 5.0952\n",
      "[Predicted] P_HILL_SPHERE        : 0.0448\n",
      "[Predicted] P_DISTANCE           : 0.9557\n",
      "[Predicted] P_PERIASTRON         : 0.8755\n",
      "[Predicted] P_APASTRON           : 0.9703\n",
      "[Predicted] P_DISTANCE_EFF       : 0.9257\n",
      "[Predicted] P_FLUX               : 1.3422\n",
      "[Predicted] P_FLUX_MIN           : 0.9797\n",
      "[Predicted] P_FLUX_MAX           : 2.1260\n",
      "[Predicted] P_TEMP_EQUIL_MIN     : 256.4579\n",
      "[Predicted] P_TEMP_EQUIL_MAX     : 296.1316\n",
      "[Predicted] P_TEMP_SURF_MIN      : 288.8159\n",
      "[Predicted] P_TEMP_SURF_MAX      : 325.4267\n",
      "[Predicted] S_LUMINOSITY         : 1.2602\n",
      "[Predicted] S_HZ_OPT_MIN         : 0.8148\n",
      "[Predicted] S_HZ_OPT_MAX         : 1.8770\n",
      "[Predicted] S_HZ_CON_MIN         : 1.0706\n",
      "[Predicted] S_HZ_CON_MAX         : 1.8534\n",
      "[Predicted] S_HZ_CON0_MIN        : 1.1140\n",
      "[Predicted] S_HZ_CON0_MAX        : 1.8534\n",
      "[Predicted] S_HZ_CON1_MIN        : 0.9853\n",
      "[Predicted] S_HZ_CON1_MAX        : 1.8534\n",
      "[Predicted] S_SNOW_LINE          : 2.8527\n",
      "[Predicted] S_TIDAL_LOCK         : 0.4531\n",
      "[Predicted] P_HABZONE_OPT        : 1.1643\n",
      "[Predicted] P_HABZONE_CON        : 0.2012\n",
      "[Predicted] P_HABITABLE          : 1.0172\n",
      "[Predicted] P_ESI                : 0.6953\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle # Added for saving/loading\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Log-transform function (Defined globally) ---\n",
    "def log_transform(df, return_logged=False):\n",
    "    df_log = df.copy()\n",
    "    logged_columns = []\n",
    "    for col in df_log.columns:\n",
    "        # Check if > 70% of data is positive AND all data is positive\n",
    "        if (df[col] > 0).sum() / len(df) > 0.7 and (df[col] > 0).all(): \n",
    "            df_log[col] = np.log1p(df[col])\n",
    "            logged_columns.append(col)\n",
    "    if return_logged: return df_log, logged_columns\n",
    "    return df_log\n",
    "\n",
    "# --- 1. Data Loading and Synthetic Fallback ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv('hwc.csv')\n",
    "    print(f\"CSV loaded: Shape {data.shape}\")\n",
    "    if data.empty:\n",
    "        raise ValueError(\"CSV empty—check file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'hwc.csv' not found—using synthetic fallback.\")\n",
    "    data = None\n",
    "except Exception as e:\n",
    "    print(f\"Load error ({e})—using synthetic fallback.\")\n",
    "    data = None\n",
    "\n",
    "if data is not None and not data.empty:\n",
    "    unnecessary = [\n",
    "        'P_NAME', 'S_NAME', 'S_NAME_HD', 'S_NAME_HIP', 'P_YEAR', 'P_UPDATE', 'P_DETECTION', 'P_DISCOVERY_FACILITY',\n",
    "        'P_MASS_ORIGIN', 'S_MASS_ORIGIN', 'S_METALLICITY_ORIGIN', 'S_FE_H_ORIGIN',\n",
    "        'S_VELOCITY_REFERENCE', 'S_VELOCITY_METHOD', 'S_VELOCITY_FLAG', 'S_VELOCITY_QUALITY', 'S_VELOCITY_SOURCE',\n",
    "        'S_VELOCITY_RADIAL_ORIGIN', 'S_VELOCITY_RADIAL_REFERENCE', 'S_VELOCITY_RADIAL_METHOD', 'S_VELOCITY_RADIAL_FLAG',\n",
    "        'S_VELOCITY_RADIAL_QUALITY', 'S_VELOCITY_RADIAL_SOURCE', 'S_VELOCITY_PROPER_MOTION_ORIGIN',\n",
    "        'S_VELOCITY_PROPER_MOTION_REFERENCE', 'S_VELOCITY_PROPER_MOTION_METHOD', 'S_VELOCITY_PROPER_MOTION_FLAG',\n",
    "        'S_VELOCITY_PROPER_MOTION_QUALITY', 'S_VELOCITY_PROPER_MOTION_SOURCE', 'S_VELOCITY_TANGENTIAL_ORIGIN',\n",
    "        'S_VELOCITY_TANGENTIAL_REFERENCE', 'S_VELOCITY_TANGENTIAL_METHOD', 'S_VELOCITY_TANGENTIAL_FLAG',\n",
    "        'S_VELOCITY_TANGENTIAL_QUALITY', 'S_VELOCITY_TANGENTIAL_SOURCE', 'S_VELOCITY_TOTAL_ORIGIN',\n",
    "        'S_VELOCITY_TOTAL_REFERENCE', 'S_VELOCITY_TOTAL_METHOD', 'S_VELOCITY_TOTAL_FLAG', 'S_VELOCITY_TOTAL_QUALITY',\n",
    "        'S_VELOCITY_TOTAL_SOURCE', 'S_TYPE', 'S_STELLAR_SPECIES', 'S_TYPE_SPECTRAL_TYPE', 'S_RA_STR', 'S_DEC_STR'\n",
    "    ]\n",
    "    data = data.drop(columns=[col for col in unnecessary if col in data.columns])\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    data = data[numeric_columns]\n",
    "else:\n",
    "    print(\"Generating synthetic data (20 rows).\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 20\n",
    "    data = pd.DataFrame({\n",
    "        'P_PERIOD': np.random.uniform(50, 1000, n_samples), 'P_RADIUS': np.random.uniform(0.5, 5.0, n_samples), \n",
    "        'P_SEMI_MAJOR_AXIS': np.random.uniform(0.1, 10.0, n_samples), 'P_ECCENTRICITY': np.random.uniform(0, 0.5, n_samples),\n",
    "        'P_TEMP_EQUIL': np.random.uniform(100, 500, n_samples), 'S_TEMPERATURE': np.random.uniform(4000, 7000, n_samples),\n",
    "        'S_MAG': np.random.uniform(5, 15, n_samples), 'P_DETECTION': np.random.choice([0, 1], n_samples),\n",
    "        'P_INCLINATION': np.random.uniform(80, 100, n_samples), 'S_AGE': np.random.uniform(1, 10, n_samples),\n",
    "        'P_VELOCITY': np.random.uniform(0, 50, n_samples), 'P_MASS': np.random.uniform(0.1, 10.0, n_samples),\n",
    "        'P_DENSITY': np.random.uniform(0.5, 8.0, n_samples), 'S_MASS': np.random.uniform(0.5, 2.0, n_samples),\n",
    "        'P_MASS_ERROR_MIN': np.random.uniform(0.01, 0.1, n_samples), \n",
    "        'S_LOG_G': np.random.uniform(4.0, 5.0, n_samples), 'S_LOG_LUM': np.random.uniform(-0.5, 1.0, n_samples)\n",
    "    })\n",
    "    for i in range(n_samples):\n",
    "        # A simple synthetic relationship for mass\n",
    "        data.loc[i, 'P_MASS'] = 1.2 * (data.loc[i, 'P_PERIOD'] / 365)**(2/3) * data.loc[i, 'P_SEMI_MAJOR_AXIS'] * (1 - data.loc[i, 'P_ECCENTRICITY']) + np.random.normal(0, 0.3)\n",
    "    numeric_columns = data.columns.tolist()\n",
    "\n",
    "# --- 2. Data Cleaning and Imputation ---\n",
    "\n",
    "if data.empty or data.shape[1] == 0:\n",
    "    raise ValueError(\"No valid data remaining after load/generation.\")\n",
    "\n",
    "missing_pct = (data.isnull().sum() / len(data)) * 100\n",
    "high_missing_cols = missing_pct[missing_pct > 40].index.tolist()\n",
    "data = data.drop(columns=high_missing_cols)\n",
    "numeric_columns = [col for col in numeric_columns if col not in high_missing_cols]\n",
    "data = data.fillna(data.mean(numeric_only=True))\n",
    "\n",
    "# --- 3. Feature/Target Selection (WITH COMPOSITION HINT) ---\n",
    "\n",
    "input_candidates = ['P_PERIOD', 'P_RADIUS', 'P_TEMP_EQUIL', 'S_TEMPERATURE', 'S_MASS']\n",
    "input_columns = [col for col in input_candidates if col in numeric_columns]\n",
    "\n",
    "all_cols_for_target_selection = [col for col in numeric_columns if col not in input_columns]\n",
    "unstable_targets = [col for col in all_cols_for_target_selection if 'LIMIT' in col or 'ERROR' in col]\n",
    "unstable_targets.extend(['S_RA', 'S_DEC', 'S_METALLICITY', 'S_ABIO_ZONE', 'P_DETECTION', 'S_MAG', 'S_AGE']) \n",
    "unstable_targets = list(set(unstable_targets)) \n",
    "\n",
    "target_columns = [col for col in all_cols_for_target_selection if col not in unstable_targets]\n",
    "\n",
    "if len(input_columns) < 3 or len(target_columns) == 0:\n",
    "    raise ValueError(\"Not enough inputs or stable targets to train the model.\")\n",
    "\n",
    "print(f\"\\n--- Model Configuration ---\")\n",
    "print(f\"Inputs (Minimal Initial + Composition Hint, {len(input_columns) + 1}): {input_columns} + P_COMPOSITION_ROCKY\")\n",
    "print(f\"Targets (Derived/Hard, {len(target_columns)}): P_ECCENTRICITY is included. Other examples: {target_columns[:5]}...\")\n",
    "\n",
    "# --- 4. Preprocessing and Feature Engineering ---\n",
    "\n",
    "data_clipped = data.copy()\n",
    "for col in numeric_columns:\n",
    "    if data[col].notna().sum() > len(data) * 0.5:\n",
    "        q99 = data[col].quantile(0.99)\n",
    "        data_clipped[col] = np.clip(data_clipped[col], None, q99)\n",
    "\n",
    "X_base = data_clipped[input_columns].apply(pd.to_numeric, errors='coerce').fillna(data_clipped[input_columns].mean())\n",
    "y_base = data_clipped[target_columns].apply(pd.to_numeric, errors='coerce').fillna(data_clipped[target_columns].mean())\n",
    "\n",
    "# Add P_COMPOSITION_ROCKY\n",
    "if 'P_DENSITY' in data_clipped.columns:\n",
    "    X_base['P_COMPOSITION_ROCKY'] = (data_clipped['P_DENSITY'] >= 3.0).astype(int)\n",
    "    input_columns.append('P_COMPOSITION_ROCKY')\n",
    "    print(\"Added P_COMPOSITION_ROCKY to training data based on P_DENSITY.\")\n",
    "else:\n",
    "    # If P_DENSITY is missing, we must remove 'P_COMPOSITION_ROCKY' from the list if it was assumed.\n",
    "    # For synthetic data, P_DENSITY exists, but this is a safety measure.\n",
    "    print(\"Warning: P_COMPOSITION_ROCKY not added due to missing P_DENSITY in dataset.\")\n",
    "\n",
    "# Feature Engineering: Proxies\n",
    "if 'P_TEMP_EQUIL' in data_clipped.columns:\n",
    "    water_proxy = (data_clipped['P_TEMP_EQUIL'] >= 200) & (data_clipped['P_TEMP_EQUIL'] <= 400)\n",
    "    X_base['water_potential_proxy'] = water_proxy.astype(int)\n",
    "    if 'water_potential_proxy' not in input_columns: input_columns.append('water_potential_proxy')\n",
    "    \n",
    "# Log-transform\n",
    "X = log_transform(X_base[input_columns], return_logged=False)\n",
    "y, logged_columns = log_transform(y_base, return_logged=True)\n",
    "\n",
    "X = X.dropna(thresh=X.shape[1] * 0.7)\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Train/Test Split and Scaling\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# --- 5. Model Training and Saving to ONE .pkl File ---\n",
    "\n",
    "print(\"\\nTraining LightGBM MultiOutputRegressor (Fast Mode)...\")\n",
    "best_lgb = LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1, n_estimators=150, learning_rate=0.08)\n",
    "model = MultiOutputRegressor(best_lgb) \n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Store all necessary components in a single dictionary\n",
    "ml_system = {\n",
    "    'model': model,\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y,\n",
    "    'X_train_columns': X_train.columns.tolist(), \n",
    "    'target_columns': target_columns, \n",
    "    'logged_columns': logged_columns\n",
    "}\n",
    "\n",
    "# Save the entire dictionary to a single file\n",
    "try:\n",
    "    with open('exoplanet_predictor_system.pkl', 'wb') as file:\n",
    "        pickle.dump(ml_system, file)\n",
    "    print(\"✅ System components saved to ONE file: 'exoplanet_predictor_system.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "\n",
    "# --- 6. Prediction Section (Fully Compatible with .pkl File) ---\n",
    "\n",
    "def load_system(filename='exoplanet_predictor_system.pkl'):\n",
    "    \"\"\"Loads all model components from the saved .pkl file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            system = pickle.load(file)\n",
    "        print(f\"\\n✅ Loaded model system from '{filename}'\")\n",
    "        print(f\"Expected {len(system['X_train_columns'])} input features:\")\n",
    "        print(system['X_train_columns'])\n",
    "        return system\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load system from {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_exoplanet_properties(new_input_dict, system_dict):\n",
    "    \"\"\"Predicts exoplanet properties with auto feature alignment.\"\"\"\n",
    "    trained_model = system_dict['model']\n",
    "    scaler_x = system_dict['scaler_X']\n",
    "    scaler_y = system_dict['scaler_y']\n",
    "    X_train_original_cols = system_dict['X_train_columns']\n",
    "    logged_cols = system_dict['logged_columns']\n",
    "    target_cols = system_dict['target_columns']\n",
    "\n",
    "    new_data_raw = pd.DataFrame({k: [v] for k, v in new_input_dict.items()})\n",
    "    X_new = pd.DataFrame(index=[0], columns=X_train_original_cols)\n",
    "\n",
    "    # --- Auto-fill all required features ---\n",
    "    missing_cols = []\n",
    "    for col in X_train_original_cols:\n",
    "        if col in new_data_raw.columns:\n",
    "            X_new[col] = new_data_raw[col]\n",
    "        elif col == 'water_potential_proxy' and 'P_TEMP_EQUIL' in new_data_raw.columns:\n",
    "            temp_val = new_data_raw['P_TEMP_EQUIL'].iloc[0]\n",
    "            X_new[col] = int(200 <= temp_val <= 400)\n",
    "        elif col == 'P_COMPOSITION_ROCKY' and 'P_DENSITY' in new_data_raw.columns:\n",
    "            X_new[col] = int(new_data_raw['P_DENSITY'].iloc[0] >= 3.0)\n",
    "        else:\n",
    "            X_new[col] = 0\n",
    "            missing_cols.append(col)\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"⚠️ Missing columns (auto-filled with 0): {missing_cols}\")\n",
    "\n",
    "    X_new = X_new.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    X_new_logged = log_transform(X_new)\n",
    "    X_new_scaled = scaler_x.transform(X_new_logged)\n",
    "\n",
    "    # --- Prediction ---\n",
    "    y_pred_scaled = trained_model.predict(X_new_scaled)\n",
    "    y_pred_orig = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    y_new_pred_df = pd.DataFrame(y_pred_orig, columns=target_cols)\n",
    "\n",
    "    # --- Reverse log-transform for logged targets ---\n",
    "    for col in logged_cols:\n",
    "        if col in y_new_pred_df.columns:\n",
    "            y_new_pred_df[col] = np.expm1(np.clip(y_new_pred_df[col], a_min=0, a_max=None))\n",
    "\n",
    "    return X_new, y_new_pred_df\n",
    "\n",
    "\n",
    "# --- 7. Accuracy Evaluation ---\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_scaled)\n",
    "y_pred_orig = scaler_y.inverse_transform(y_pred_scaled)\n",
    "r2_values = r2_score(y_test_orig, y_pred_orig, multioutput='raw_values')\n",
    "avg_r2 = np.mean(r2_values[r2_values >= -1])\n",
    "print(\"\\n=== Model Accuracy Evaluation ===\")\n",
    "print(f\"Overall Avg R²: {avg_r2:.4f}\")\n",
    "\n",
    "\n",
    "# --- 8. User Prediction Input ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"USER-EDITABLE PREDICTION INPUTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "initial_discovery_inputs = {\n",
    "    'P_PERIOD': 300,            # Orbital Period (days)\n",
    "    'P_RADIUS': 1.400,           # Planet Radius (Earth radii)\n",
    "    'P_TEMP_EQUIL': 270.0,     # Equilibrium Temperature (K)\n",
    "    'S_TEMPERATURE': 5778.0,    # Star Temperature (K)\n",
    "    'S_MASS': 1,              # Star Mass (Solar masses)\n",
    "    'P_COMPOSITION_ROCKY': 1.0  # 1.0 = Rocky, 0.0 = Gaseous\n",
    "}\n",
    "\n",
    "# Load trained system and predict\n",
    "system = load_system()\n",
    "if system:\n",
    "    X_new_df, y_pred_df = predict_exoplanet_properties(initial_discovery_inputs, system)\n",
    "else:\n",
    "    print(\"⚠️ Could not load system. Using in-memory objects instead.\")\n",
    "    system = {\n",
    "        'model': model,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'X_train_columns': X_train.columns.tolist(),\n",
    "        'logged_columns': logged_columns,\n",
    "        'target_columns': target_columns\n",
    "    }\n",
    "    X_new_df, y_pred_df = predict_exoplanet_properties(initial_discovery_inputs, system)\n",
    "\n",
    "\n",
    "# --- 9. Display Final Prediction Report ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"PREDICTION REPORT | Overall Model R²: {avg_r2:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "input_vals = X_new_df.iloc[0].round(4).to_dict()\n",
    "output_vals = y_pred_df.iloc[0].round(4).to_dict()\n",
    "max_key_len = max(len(k) for k in (list(input_vals.keys()) + list(output_vals.keys())))\n",
    "\n",
    "print(\"--- INPUTS USED ---\")\n",
    "for k, v in input_vals.items():\n",
    "    print(f\"[Input]     {k:<{max_key_len}}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n--- PREDICTED OUTPUTS ---\")\n",
    "for k, v in output_vals.items():\n",
    "    print(f\"[Predicted] {k:<{max_key_len}}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
